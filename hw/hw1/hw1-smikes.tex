\documentclass[12pt,letterpaper]{hmcpset}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{hyperref}

\input{macros.tex}

% info for header block in upper right hand corner
\name{}
\class{Math189R SU20.1}
\assignment{Homework 1}
\duedate{Thursday, June 11, 2020}

\renewcommand{\labelenumi}{{(\alph{enumi})}}


\begin{document}

\begin{problem}[1]
(\textbf{Linear Transformation}) Let $\mathbf{y} = A\mathbf{x} + \mathbf{b}$ be a random vector.
show that expectation is linear:
\[
    \EE[\yy] = \EE[A\xx + \bb] = A\EE[\xx] + \bb.
\]
Also show that
\[
    \cov[\yy] = \cov[A\xx + \bb] = A \cov[\xx] A^\T = A\Sigmab A^\T.
\]
\end{problem}
\begin{solution}

  Expectation for a continuous random variable $x$ with normalized probability distribution function $f_x(x)$ is defined as

\[
  \EE[x]= \int_{-\infty}^{\infty} xf_X(x)dx
\]

  Consider first a single random variable $x$, and $y = ax + b$
\[
\EE[x] = \EE[ax + b]
         = \int_{-\infty}^{\infty} (ax + b) f_X(x) dx
         = a\int_{-\infty}^{\infty} x f_X(x) dx + b \int_{-\infty}^{\infty} f_X(x) dx
         = a\EE[x] + b
\]

In multiple variables, the $x_i$ are independent, so it follows that
\[
   \EE[A\xx + \bb] = A\EE[\xx] + \bb
\]

Covariance is defined
\[
  \CC[\xx] = \EE[(X - \EE[X])(X - \EE[X])^T]
\]

Again, since the expectation $\EE$ is linear in $\xx$, the covariance and any linear transformation of $\xx$ commute and can be rearranged.  The term in $\mathbf{b}$ vanishes because the covariance of a constant term is zero.
\[
  \yy = A\xx + \bb
\]

\[
  \CC(\yy) = \CC(A\xx + \bb)
           = A\CC(x)A^T
\]

Works used: \url{https://www.probabilitycourse.com/chapter6/6_1_5_random_vectors.php}

    \vfill
\end{solution}
\newpage




\begin{problem}[2]
Given the dataset $\Dc = \{(x,y)\} = \{(0,1), (2,3), (3,6), (4,8)\}$
\begin{enumerate}
   \item Find the least squares estimate $y = \thetab^\T\xx$ by hand using
        Cramer's Rule.
    \item Use the normal equations to find the same solution and verify it
        is the same as part (a).
    \item Plot the data and the optimal linear fit you found.
    \item Find randomly generate 100 points near the line with white Gaussian
        noise and then compute the least squares estimate (using a computer).
        Verify that this new line is close to the original and plot the new
        dataset, the old line, and the new line.
\end{enumerate}

\end{problem}
\begin{solution}
1. By Cramer's rule, we have the closed forms
\[
    m = \frac{n\sum_{i=1}^n x_iy_i - (\sum_{i=1}^n x_i)(\sum_{i=1}^n y_i)}{n\sum_{i=1}^n x_i^2 - (\sum_{i=1}^n x_i)^2}
\]
\[
    b = \frac{n\sum_{i=1}^n x_i^2y_i - (\sum_{i=1}^n x_i)(\sum_{i=1}^n x_iy_i)}{n\sum_{i=1}^n x_i^2 - (\sum_{i=1}^n x_i)^2}
\]
Evaluation gives
\[
\sum(x) = 9, \sum(y) = 18, \sum{x^2} = 29, \sum(xy) = 56, \sum(x^2y) = 194
\]
Then
\[
m = \frac{(4*56 - 9*18)}{4*29 - 81}
= \frac{62}{35} =~ 1.77
\]
and
\[
b = \frac{194}{35} =~ 5.54
\]

2. Minimize the error, where

\[
 error = \ee \cdot \ee
\]

\[
 \ee = \yy - ( m \xx + b )
\]

\[
 \ee =
 \begin{bmatrix}
   1 \\
   3 \\
   6 \\
   8 \\
\end{bmatrix}
 - m *
\begin{bmatrix}
  0 \\
  2 \\
  3 \\
  4 \\
\end{bmatrix}
 - b
\]

    \vfill
\end{solution}
\newpage



\end{document}
